from sys import argv
from os import path
from collections import defaultdict
from nltk.stem.porter import PorterStemmer
from itertools import islice

# All stop_words dictionaries should be generated by this function to ensure the strings are massaged the same way as
# the strings they'll be compared to in this function
def convert_file_to_stemmed_word_count_dict(file_path, stop_words=defaultdict(int)):
    word_counts = defaultdict(int)
    stemmer = PorterStemmer()
    
    # prevent other users(the test suite) from passing in a dictionary without defaults
    # the defaultdict allows this function to not worry about KeyErrors when a key hasn't been entered yet
    if not isinstance(stop_words, defaultdict):
        safe_stop_words = defaultdict(int)
        for k,v in stop_words.items():
            safe_stop_words[k] = v
        stop_words = safe_stop_words

    # short circuit on bas input
    # there are a few other things worth checking for here that I chose not to pursue at the time:
    #       sym links
    #       disk mounts
    if not path.exists(file_path) or path.isdir(file_path):
        return word_counts
    
    # extract all words at once to minimize time with the file open
    # we could probably save what amounts to a trip through the text by running the next block in here
    # but we're already in linear time. Improving this could be considered if performance was that critical or the rest
    # of this file reached log time or better.
    lines = None
    with open(file_path, 'r') as reader:
        lines = reader.readlines()
    

    # note: This will be innaccurate in cases such as ```Alice)--`and```, where this will be treated as the word aliceand.
    # with the limited test data and time, it doesn't seem worth it to work around this edge case
    for line in lines:
        # clean off whitespace, drop all non-alphabetical characters
        for word in line.split():
            # I checked the performance of this versus using a replacement regex. It seems similar when the regex
            # engine caches(?) the expression, but before that this is around 100-1000x faster on my machine
            alpha_word = ''.join([i for i in word if i.isalpha()]).lower()
            stemmed_word = stemmer.stem(alpha_word)
            if stemmed_word and not stop_words[stemmed_word]:
                word_counts[stemmed_word] += 1

    return word_counts

def get_top_20_word_count(word_dict):
    # fun fact: normally you'd to choose a generator over a list comprehension here to save on space and time,
    # but sorting the list causes a new list allocation anyway.
    sorted_words = [k for k, v in sorted(word_dict.items(), key=lambda item: item[1], reverse=True)]
    return list(islice(sorted_words, 20))

def main():
    # grab the first two arguments. if this becomes more complicated, consider using getopt for options/flags
    stop_words_file = argv[1]
    text_file = argv[2]

    stop_words = convert_file_to_stemmed_word_count_dict(stop_words_file)
    text_words = convert_file_to_stemmed_word_count_dict(text_file, stop_words=stop_words)

    top_20 = get_top_20_word_count(text_words)

    print(top_20)

if __name__ == "__main__":
    main()